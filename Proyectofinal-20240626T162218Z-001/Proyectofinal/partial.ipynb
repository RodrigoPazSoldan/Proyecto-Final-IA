{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1xLMPis5nEMnLYjMd3tHFMwoJ7h2pumNh","authorship_tag":"ABX9TyNDAjmfolNS3miBLDXEOAKL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6hMLreN-7EtU","outputId":"b970ed85-2b8b-4da1-a8b8-4c23ed11b98f","executionInfo":{"status":"ok","timestamp":1719418772322,"user_tz":240,"elapsed":58291,"user":{"displayName":"Eliotness01","userId":"05711681705704863254"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-11-33a594e8d3a3>:82: DtypeWarning: Columns (1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/sentiment.csv', encoding='latin-1')\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["116866    1\n","38506     1\n","62008     1\n","106787    1\n","66720     1\n","         ..\n","141398    0\n","14043     0\n","10130     0\n","71123     0\n","103270    0\n","Name: Sentiment_labeled, Length: 150, dtype: int64\n","Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," lstm_6 (LSTM)               (None, 42, 100)           40800     \n","                                                                 \n"," dropout_6 (Dropout)         (None, 42, 100)           0         \n","                                                                 \n"," lstm_7 (LSTM)               (None, 100)               80400     \n","                                                                 \n"," dropout_7 (Dropout)         (None, 100)               0         \n","                                                                 \n"," dense_3 (Dense)             (None, 3)                 303       \n","                                                                 \n","=================================================================\n","Total params: 121503 (474.62 KB)\n","Trainable params: 121503 (474.62 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/100\n","4/4 [==============================] - 6s 404ms/step - loss: 1.1080 - accuracy: 0.3083 - val_loss: 1.0912 - val_accuracy: 0.2667\n","Epoch 2/100\n","4/4 [==============================] - 0s 79ms/step - loss: 1.0859 - accuracy: 0.3417 - val_loss: 1.0895 - val_accuracy: 0.3000\n","Epoch 3/100\n","4/4 [==============================] - 0s 86ms/step - loss: 1.0774 - accuracy: 0.3917 - val_loss: 1.0831 - val_accuracy: 0.2333\n","Epoch 4/100\n","4/4 [==============================] - 0s 84ms/step - loss: 1.0688 - accuracy: 0.4167 - val_loss: 1.0765 - val_accuracy: 0.3333\n","Epoch 5/100\n","4/4 [==============================] - 0s 80ms/step - loss: 1.0491 - accuracy: 0.4250 - val_loss: 1.0704 - val_accuracy: 0.3667\n","Epoch 6/100\n","4/4 [==============================] - 0s 83ms/step - loss: 1.0489 - accuracy: 0.4333 - val_loss: 1.0582 - val_accuracy: 0.4000\n","Epoch 7/100\n","4/4 [==============================] - 0s 83ms/step - loss: 1.0359 - accuracy: 0.4000 - val_loss: 1.0508 - val_accuracy: 0.3667\n","Epoch 8/100\n","4/4 [==============================] - 0s 78ms/step - loss: 1.0437 - accuracy: 0.4250 - val_loss: 1.0381 - val_accuracy: 0.4667\n","Epoch 9/100\n","4/4 [==============================] - 0s 79ms/step - loss: 1.0276 - accuracy: 0.4583 - val_loss: 1.0291 - val_accuracy: 0.4000\n","Epoch 10/100\n","4/4 [==============================] - 0s 84ms/step - loss: 1.0135 - accuracy: 0.4333 - val_loss: 1.0219 - val_accuracy: 0.4333\n","Epoch 11/100\n","4/4 [==============================] - 0s 80ms/step - loss: 1.0040 - accuracy: 0.4833 - val_loss: 1.0079 - val_accuracy: 0.5333\n","Epoch 12/100\n","4/4 [==============================] - 0s 81ms/step - loss: 0.9943 - accuracy: 0.4500 - val_loss: 0.9756 - val_accuracy: 0.5333\n","Epoch 13/100\n","4/4 [==============================] - 0s 97ms/step - loss: 1.0007 - accuracy: 0.4917 - val_loss: 0.9425 - val_accuracy: 0.5000\n","Epoch 14/100\n","4/4 [==============================] - 0s 81ms/step - loss: 0.9721 - accuracy: 0.4833 - val_loss: 0.9083 - val_accuracy: 0.6333\n","Epoch 15/100\n","4/4 [==============================] - 0s 83ms/step - loss: 0.9340 - accuracy: 0.4500 - val_loss: 0.8894 - val_accuracy: 0.5000\n","Epoch 16/100\n","4/4 [==============================] - 0s 92ms/step - loss: 0.9294 - accuracy: 0.5500 - val_loss: 0.8732 - val_accuracy: 0.6000\n","Epoch 17/100\n","4/4 [==============================] - 0s 87ms/step - loss: 0.9721 - accuracy: 0.5417 - val_loss: 0.9736 - val_accuracy: 0.4667\n","Epoch 18/100\n","4/4 [==============================] - 0s 80ms/step - loss: 0.9573 - accuracy: 0.5250 - val_loss: 0.9480 - val_accuracy: 0.4667\n","Epoch 19/100\n","4/4 [==============================] - 0s 96ms/step - loss: 0.9515 - accuracy: 0.5417 - val_loss: 0.9580 - val_accuracy: 0.4333\n","Epoch 20/100\n","4/4 [==============================] - 0s 81ms/step - loss: 0.9321 - accuracy: 0.5333 - val_loss: 0.9526 - val_accuracy: 0.5333\n","Epoch 21/100\n","4/4 [==============================] - 0s 94ms/step - loss: 0.9502 - accuracy: 0.5250 - val_loss: 0.8694 - val_accuracy: 0.6000\n","Epoch 22/100\n","4/4 [==============================] - 0s 92ms/step - loss: 0.9153 - accuracy: 0.5250 - val_loss: 0.8729 - val_accuracy: 0.5333\n","Epoch 23/100\n","4/4 [==============================] - 0s 80ms/step - loss: 0.8988 - accuracy: 0.5667 - val_loss: 0.9110 - val_accuracy: 0.5667\n","Epoch 24/100\n","4/4 [==============================] - 0s 105ms/step - loss: 0.9196 - accuracy: 0.5667 - val_loss: 0.9377 - val_accuracy: 0.4667\n","Epoch 25/100\n","4/4 [==============================] - 0s 132ms/step - loss: 0.8940 - accuracy: 0.5583 - val_loss: 0.8739 - val_accuracy: 0.5667\n","Epoch 26/100\n","4/4 [==============================] - 0s 123ms/step - loss: 0.8773 - accuracy: 0.6333 - val_loss: 0.8262 - val_accuracy: 0.6000\n","Epoch 27/100\n","4/4 [==============================] - 1s 147ms/step - loss: 0.8640 - accuracy: 0.5833 - val_loss: 0.8935 - val_accuracy: 0.5333\n","Epoch 28/100\n","4/4 [==============================] - 0s 125ms/step - loss: 0.9163 - accuracy: 0.5667 - val_loss: 0.9461 - val_accuracy: 0.5333\n","Epoch 29/100\n","4/4 [==============================] - 1s 129ms/step - loss: 0.9013 - accuracy: 0.5750 - val_loss: 0.8643 - val_accuracy: 0.5333\n","Epoch 30/100\n","4/4 [==============================] - 1s 137ms/step - loss: 0.8785 - accuracy: 0.5500 - val_loss: 0.7694 - val_accuracy: 0.7000\n","Epoch 31/100\n","4/4 [==============================] - 1s 139ms/step - loss: 0.8780 - accuracy: 0.5917 - val_loss: 0.7719 - val_accuracy: 0.6667\n","Epoch 32/100\n","4/4 [==============================] - 1s 152ms/step - loss: 0.8400 - accuracy: 0.6167 - val_loss: 0.9331 - val_accuracy: 0.5333\n","Epoch 33/100\n","4/4 [==============================] - 1s 138ms/step - loss: 0.8380 - accuracy: 0.6167 - val_loss: 0.9039 - val_accuracy: 0.5333\n","Epoch 34/100\n","4/4 [==============================] - 0s 88ms/step - loss: 0.8201 - accuracy: 0.6583 - val_loss: 0.8185 - val_accuracy: 0.5667\n","Epoch 35/100\n","4/4 [==============================] - 0s 81ms/step - loss: 0.8424 - accuracy: 0.6083 - val_loss: 0.8429 - val_accuracy: 0.5333\n","Epoch 36/100\n","4/4 [==============================] - 0s 88ms/step - loss: 0.8585 - accuracy: 0.5833 - val_loss: 0.9204 - val_accuracy: 0.5333\n","Epoch 37/100\n","4/4 [==============================] - 0s 82ms/step - loss: 0.8344 - accuracy: 0.6250 - val_loss: 0.8474 - val_accuracy: 0.6000\n","Epoch 38/100\n","4/4 [==============================] - 0s 79ms/step - loss: 0.8337 - accuracy: 0.6167 - val_loss: 0.8118 - val_accuracy: 0.5000\n","Epoch 39/100\n","4/4 [==============================] - 0s 87ms/step - loss: 0.9341 - accuracy: 0.5583 - val_loss: 0.8737 - val_accuracy: 0.6667\n","Epoch 40/100\n","4/4 [==============================] - 0s 82ms/step - loss: 0.9805 - accuracy: 0.4833 - val_loss: 1.0033 - val_accuracy: 0.5000\n","Epoch 41/100\n","4/4 [==============================] - 0s 81ms/step - loss: 0.8852 - accuracy: 0.5500 - val_loss: 1.0394 - val_accuracy: 0.5000\n","Epoch 42/100\n","4/4 [==============================] - 0s 87ms/step - loss: 0.9726 - accuracy: 0.4917 - val_loss: 0.8988 - val_accuracy: 0.5000\n","Epoch 43/100\n","4/4 [==============================] - 0s 97ms/step - loss: 0.9039 - accuracy: 0.5667 - val_loss: 0.9788 - val_accuracy: 0.5333\n","Epoch 44/100\n","4/4 [==============================] - 0s 84ms/step - loss: 0.9189 - accuracy: 0.5667 - val_loss: 0.8854 - val_accuracy: 0.6667\n","Epoch 45/100\n","4/4 [==============================] - 0s 85ms/step - loss: 0.8620 - accuracy: 0.6167 - val_loss: 0.8423 - val_accuracy: 0.6000\n","Epoch 46/100\n","4/4 [==============================] - 0s 95ms/step - loss: 0.8597 - accuracy: 0.5583 - val_loss: 0.8447 - val_accuracy: 0.6333\n","Epoch 47/100\n","4/4 [==============================] - 0s 81ms/step - loss: 0.8327 - accuracy: 0.6083 - val_loss: 0.9235 - val_accuracy: 0.5000\n","Epoch 48/100\n","4/4 [==============================] - 0s 100ms/step - loss: 0.8253 - accuracy: 0.6167 - val_loss: 0.9367 - val_accuracy: 0.5333\n","Epoch 49/100\n","4/4 [==============================] - 0s 87ms/step - loss: 0.8075 - accuracy: 0.6333 - val_loss: 0.9234 - val_accuracy: 0.5333\n","Epoch 50/100\n","4/4 [==============================] - 0s 81ms/step - loss: 0.7945 - accuracy: 0.6417 - val_loss: 0.7909 - val_accuracy: 0.6333\n","Epoch 51/100\n","4/4 [==============================] - 0s 95ms/step - loss: 0.7817 - accuracy: 0.6583 - val_loss: 0.8964 - val_accuracy: 0.5333\n","Epoch 52/100\n","4/4 [==============================] - 0s 96ms/step - loss: 0.7952 - accuracy: 0.6500 - val_loss: 0.7956 - val_accuracy: 0.5667\n","Epoch 53/100\n","4/4 [==============================] - 0s 82ms/step - loss: 0.7729 - accuracy: 0.6333 - val_loss: 0.9078 - val_accuracy: 0.5333\n","Epoch 54/100\n","4/4 [==============================] - 0s 99ms/step - loss: 0.7062 - accuracy: 0.7083 - val_loss: 0.7945 - val_accuracy: 0.6000\n","Epoch 55/100\n","4/4 [==============================] - 0s 91ms/step - loss: 0.7809 - accuracy: 0.6333 - val_loss: 0.8341 - val_accuracy: 0.5000\n","Epoch 56/100\n","4/4 [==============================] - 0s 98ms/step - loss: 0.9188 - accuracy: 0.5833 - val_loss: 1.1821 - val_accuracy: 0.4333\n","Epoch 57/100\n","4/4 [==============================] - 0s 97ms/step - loss: 0.9156 - accuracy: 0.5417 - val_loss: 1.1011 - val_accuracy: 0.4667\n","Epoch 58/100\n","4/4 [==============================] - 1s 175ms/step - loss: 0.8503 - accuracy: 0.5833 - val_loss: 0.9400 - val_accuracy: 0.6000\n","Epoch 59/100\n","4/4 [==============================] - 0s 88ms/step - loss: 0.8557 - accuracy: 0.6000 - val_loss: 0.8623 - val_accuracy: 0.6000\n","Epoch 60/100\n","4/4 [==============================] - 1s 203ms/step - loss: 0.8435 - accuracy: 0.6083 - val_loss: 0.9302 - val_accuracy: 0.5667\n","Epoch 61/100\n","4/4 [==============================] - 1s 143ms/step - loss: 0.7998 - accuracy: 0.6333 - val_loss: 0.9924 - val_accuracy: 0.4333\n","Epoch 62/100\n","4/4 [==============================] - 1s 275ms/step - loss: 0.7704 - accuracy: 0.6917 - val_loss: 0.9906 - val_accuracy: 0.5667\n","Epoch 63/100\n","4/4 [==============================] - 1s 137ms/step - loss: 0.7345 - accuracy: 0.6833 - val_loss: 0.8857 - val_accuracy: 0.6667\n","Epoch 64/100\n","4/4 [==============================] - 1s 144ms/step - loss: 0.7123 - accuracy: 0.6833 - val_loss: 0.9209 - val_accuracy: 0.5667\n","Epoch 65/100\n","4/4 [==============================] - 1s 149ms/step - loss: 0.7231 - accuracy: 0.6417 - val_loss: 0.9591 - val_accuracy: 0.5667\n","Epoch 66/100\n","4/4 [==============================] - 1s 156ms/step - loss: 0.7507 - accuracy: 0.6583 - val_loss: 1.0409 - val_accuracy: 0.6000\n","Epoch 67/100\n","4/4 [==============================] - 1s 141ms/step - loss: 0.6931 - accuracy: 0.6667 - val_loss: 0.7769 - val_accuracy: 0.6667\n","Epoch 68/100\n","4/4 [==============================] - 1s 128ms/step - loss: 0.6851 - accuracy: 0.6917 - val_loss: 0.8376 - val_accuracy: 0.5000\n","Epoch 69/100\n","4/4 [==============================] - 0s 80ms/step - loss: 0.6772 - accuracy: 0.6583 - val_loss: 0.7869 - val_accuracy: 0.5667\n","Epoch 70/100\n","4/4 [==============================] - 0s 92ms/step - loss: 0.6476 - accuracy: 0.7333 - val_loss: 1.2047 - val_accuracy: 0.5667\n","Epoch 71/100\n","4/4 [==============================] - 0s 94ms/step - loss: 0.7623 - accuracy: 0.6417 - val_loss: 1.3602 - val_accuracy: 0.5000\n","Epoch 72/100\n","4/4 [==============================] - 0s 81ms/step - loss: 1.0578 - accuracy: 0.5333 - val_loss: 1.3829 - val_accuracy: 0.3333\n","Epoch 73/100\n","4/4 [==============================] - 0s 98ms/step - loss: 0.9881 - accuracy: 0.5500 - val_loss: 1.1782 - val_accuracy: 0.4333\n","Epoch 74/100\n","4/4 [==============================] - 0s 81ms/step - loss: 0.8288 - accuracy: 0.5333 - val_loss: 1.1094 - val_accuracy: 0.4667\n","Epoch 75/100\n","4/4 [==============================] - 0s 94ms/step - loss: 0.9035 - accuracy: 0.5500 - val_loss: 1.0747 - val_accuracy: 0.4667\n","Epoch 76/100\n","4/4 [==============================] - 0s 103ms/step - loss: 0.8358 - accuracy: 0.6000 - val_loss: 1.0656 - val_accuracy: 0.5333\n","Epoch 77/100\n","4/4 [==============================] - 0s 87ms/step - loss: 0.7869 - accuracy: 0.6000 - val_loss: 1.0243 - val_accuracy: 0.5000\n","Epoch 78/100\n","4/4 [==============================] - 0s 80ms/step - loss: 0.7872 - accuracy: 0.6250 - val_loss: 1.0101 - val_accuracy: 0.4667\n","Epoch 79/100\n","4/4 [==============================] - 0s 89ms/step - loss: 0.7625 - accuracy: 0.6250 - val_loss: 0.9520 - val_accuracy: 0.5333\n","Epoch 80/100\n","4/4 [==============================] - 0s 81ms/step - loss: 0.7352 - accuracy: 0.6417 - val_loss: 0.9047 - val_accuracy: 0.6333\n","Epoch 81/100\n","4/4 [==============================] - 0s 82ms/step - loss: 0.6748 - accuracy: 0.7000 - val_loss: 0.8517 - val_accuracy: 0.6333\n","Epoch 82/100\n","4/4 [==============================] - 0s 88ms/step - loss: 0.6786 - accuracy: 0.7083 - val_loss: 0.8717 - val_accuracy: 0.6667\n","Epoch 83/100\n","4/4 [==============================] - 0s 82ms/step - loss: 0.6417 - accuracy: 0.7333 - val_loss: 0.8606 - val_accuracy: 0.6000\n","Epoch 84/100\n","4/4 [==============================] - 0s 84ms/step - loss: 0.6227 - accuracy: 0.7417 - val_loss: 0.8198 - val_accuracy: 0.6333\n","Epoch 85/100\n","4/4 [==============================] - 0s 82ms/step - loss: 0.6575 - accuracy: 0.6750 - val_loss: 0.9752 - val_accuracy: 0.6000\n","Epoch 86/100\n","4/4 [==============================] - 0s 93ms/step - loss: 0.6545 - accuracy: 0.7083 - val_loss: 0.9804 - val_accuracy: 0.6000\n","Epoch 87/100\n","4/4 [==============================] - 0s 83ms/step - loss: 0.6871 - accuracy: 0.7000 - val_loss: 0.9451 - val_accuracy: 0.6000\n","Epoch 88/100\n","4/4 [==============================] - 0s 84ms/step - loss: 0.6777 - accuracy: 0.6750 - val_loss: 0.9402 - val_accuracy: 0.5667\n","Epoch 89/100\n","4/4 [==============================] - 0s 81ms/step - loss: 0.6619 - accuracy: 0.6500 - val_loss: 0.7296 - val_accuracy: 0.6667\n","Epoch 90/100\n","4/4 [==============================] - 0s 79ms/step - loss: 0.6334 - accuracy: 0.7167 - val_loss: 0.8087 - val_accuracy: 0.5333\n","Epoch 91/100\n","4/4 [==============================] - 0s 83ms/step - loss: 0.6042 - accuracy: 0.7500 - val_loss: 0.7756 - val_accuracy: 0.6667\n","Epoch 92/100\n","4/4 [==============================] - 0s 94ms/step - loss: 0.6383 - accuracy: 0.7083 - val_loss: 0.8415 - val_accuracy: 0.5667\n","Epoch 93/100\n","4/4 [==============================] - 0s 97ms/step - loss: 0.6178 - accuracy: 0.7667 - val_loss: 1.0542 - val_accuracy: 0.5333\n","Epoch 94/100\n","4/4 [==============================] - 0s 83ms/step - loss: 0.6475 - accuracy: 0.7083 - val_loss: 0.7181 - val_accuracy: 0.7000\n","Epoch 95/100\n","4/4 [==============================] - 0s 94ms/step - loss: 0.6271 - accuracy: 0.6750 - val_loss: 1.0227 - val_accuracy: 0.6000\n","Epoch 96/100\n","4/4 [==============================] - 0s 85ms/step - loss: 0.6197 - accuracy: 0.6917 - val_loss: 1.1901 - val_accuracy: 0.5333\n","Epoch 97/100\n","4/4 [==============================] - 0s 81ms/step - loss: 0.6028 - accuracy: 0.7500 - val_loss: 1.0045 - val_accuracy: 0.5667\n","Epoch 98/100\n","4/4 [==============================] - 0s 112ms/step - loss: 0.5530 - accuracy: 0.7250 - val_loss: 0.8422 - val_accuracy: 0.6333\n","Epoch 99/100\n","4/4 [==============================] - 0s 123ms/step - loss: 0.5417 - accuracy: 0.7417 - val_loss: 1.0085 - val_accuracy: 0.5667\n","Epoch 100/100\n","4/4 [==============================] - 1s 142ms/step - loss: 0.5433 - accuracy: 0.7750 - val_loss: 0.8966 - val_accuracy: 0.5333\n","1/1 [==============================] - 0s 66ms/step - loss: 0.8966 - accuracy: 0.5333\n","Loss: 0.8965937495231628\n","Accuracy: 0.5333333611488342\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]}],"source":["\n","##F1\n","\n","import pandas as pd\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import string\n","import re\n","import pickle\n","\n","\n","def expand_contractions(text):\n","\n","    contractions_dict = {\n","        \"ain't\": \"is not\",\n","        \"aren't\": \"are not\",\n","        \"can't\": \"cannot\",\n","        \"can't've\": \"cannot have\",\n","        \"'cause\": \"because\",\n","        \"could've\": \"could have\",\n","        \"couldn't\": \"could not\",\n","        \"couldn't've\": \"could not have\",\n","        \"didn't\": \"did not\",\n","        \"doesn't\": \"does not\",\n","        \"don't\": \"do not\",\n","        \"hadn't\": \"had not\",\n","        \"hadn't've\": \"had not have\",\n","        \"hasn't\": \"has not\",\n","        \"haven't\": \"have not\",\n","        \"isn't\": \"is not\",\n","        \"it'd\": \"it would\",\n","        \"it'd've\": \"it would have\",\n","        \"it'll\": \"it will\",\n","        \"it'll've\": \"it will have\",\n","        \"it's\": \"it is\",\n","        \"should've\": \"should have\",\n","        \"shouldn't\": \"should not\",\n","        \"shouldn't've\": \"should not have\",\n","        \"so've\": \"so have\",\n","        \"so's\": \"so is\",\n","        \"that'd\": \"that would\",\n","        \"that'd've\": \"that would have\",\n","        \"that's\": \"that is\",\n","        \"there'd\": \"there would\",\n","        \"there'd've\": \"there would have\",\n","        \"there's\": \"there is\",\n","        \"to've\": \"to have\",\n","        \"wasn't\": \"was not\",\n","        \"weren't\": \"were not\",\n","        \"what'll\": \"what will\",\n","        \"will've\": \"will have\",\n","        \"won't\": \"will not\",\n","        \"won't've\": \"will not have\",\n","        \"would've\": \"would have\",\n","        \"wouldn't\": \"would not\",\n","        \"wouldn't've\": \"would not have\",\n","    }\n","\n","    contractions_pattern = re.compile(r'\\b(' + '|'.join(contractions_dict.keys()) + r')\\b')\n","\n","    # Expand function\n","    def expand_match(contraction):\n","        return contractions_dict[contraction.group(0)]\n","\n","    # Verify if text is str\n","    if isinstance(text, str):\n","\n","        expanded_text = contractions_pattern.sub(expand_match, text)\n","    else:\n","\n","        expanded_text = text\n","\n","    return expanded_text\n","\n","\n","\n","##F2\n","\n","#Import Dataset\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/sentiment.csv', encoding='latin-1')\n","\n","count_positivo = 0\n","count_negativo = 0\n","count_neutral = 0\n","\n","for index, row in df.iterrows():\n","    if row['Sentiment'] == 'positive':\n","        count_positivo += 1\n","    elif row['Sentiment'] == 'negative':\n","        count_negativo += 1\n","    elif row['Sentiment'] == 'neutral':\n","        count_neutral += 1\n","\n","# Number of rows for each category\n","num_per_category = 50\n","\n","df_positive = df[df['Sentiment'] == 'positive'].sample(n=num_per_category, replace=True)\n","df_negative = df[df['Sentiment'] == 'negative'].sample(n=num_per_category, replace=True)\n","df_neutral = df[df['Sentiment'] == 'neutral'].sample(n=num_per_category, replace=True)\n","\n","df_balanced = pd.concat([df_positive, df_negative, df_neutral])\n","\n","\n","\n","\n","\n","\n","##  F3\n","\n","def eliminar_filas_nan(df, columna):\n","\n","    df_sin_nan = df.dropna(subset=[columna])\n","\n","    return df_sin_nan\n","\n","df_sin_nan = eliminar_filas_nan(df_balanced, 'Summary')\n","\n","# Convert df to a list\n","Reviews = df_sin_nan['Summary'].tolist()\n","\n","\n","\n","## F4\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","import string\n","import re\n","\n","def remove_in_words(reviews):\n","    punct = set(string.punctuation)\n","    text = []\n","\n","    for paragraph in reviews:\n","        cleaned_paragraph = []\n","        for word in paragraph.split():\n","            cleaned_chars = []\n","            previous_char = None\n","            for letra in word:\n","                if letra.isdigit():\n","                    continue\n","                if letra in punct:\n","                    if letra != previous_char:\n","                        cleaned_chars.append(letra)\n","                    previous_char = letra\n","                else:\n","                    cleaned_chars.append(letra)\n","                    previous_char = letra\n","\n","            cleaned_word = ''.join(cleaned_chars)\n","            cleaned_paragraph.append(cleaned_word)\n","\n","        cleaned_paragraph = ' '.join(cleaned_paragraph)\n","        text.append(cleaned_paragraph)\n","\n","    return text\n","\n","cleaned_reviews = remove_in_words(Reviews)\n","\n","\n","\n","## F5\n","\n","\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","# Delete stopwords\n","def pre_process(reviews):\n","    stopset = set(stopwords.words('english'))\n","    # Words that does not have to delete\n","    whitelist = {'not', 'no', 'nor'}\n","\n","    processed_reviews = []\n","    for corp in reviews:\n","        corpus = corp.lower()\n","        words = word_tokenize(corpus)\n","        filtered_words = []\n","        for word in words:\n","            if word in stopset and word not in whitelist:\n","                continue\n","            filtered_words.append(word)\n","        processed_review = \" \".join(filtered_words)\n","        processed_reviews.append(processed_review)\n","\n","    return processed_reviews\n","\n","df2 = pre_process(cleaned_reviews)\n","\n","\n","\n","##F6\n","\n","sentimiento_mapping = {'positive': 1, 'negative': 2, 'neutral': 0}\n","\n","# Aplicar el mapeo a la columna 'sentimiento'\n","df_balanced['Sentiment_labeled'] = df_balanced['Sentiment'].map(sentimiento_mapping)\n","print(df_balanced['Sentiment_labeled'])\n","\n","import numpy as np\n","import pandas as pd\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout\n","\n","# Labels to categorical\n","y = df_balanced['Sentiment_labeled']\n","\n","y = to_categorical(y, num_classes=3)\n","\n","# Sequences and token\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(df2)\n","sequences = tokenizer.texts_to_sequences(df2)\n","X = pad_sequences(sequences, padding='post')\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Max lenght of the sequences\n","max_length = X.shape[1]\n","\n","# Model\n","model = Sequential()\n","model.add(LSTM(100, input_shape=(max_length, 1), return_sequences=True))\n","model.add(Dropout(0.5))\n","model.add(LSTM(100))\n","model.add(Dropout(0.5))\n","\n","model.add(Dense(3, activation='softmax'))\n","\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","model.summary()\n","\n","# Adjust the shape of data for LSTM\n","X_train = np.expand_dims(X_train, -1)\n","X_test = np.expand_dims(X_test, -1)\n","\n","# Training\n","history = model.fit(X_train, y_train, epochs=100, verbose=1, validation_data=(X_test, y_test), batch_size=32)\n","\n","# Evaluation\n","loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n","print(f'Loss: {loss}')\n","print(f'Accuracy: {accuracy}')\n","\n","import pickle\n","\n","# Guardar el modelo\n","model.save('sentiment_model.h5')  # Guardar como .h5 para compatibilidad\n","\n","# Guardar el tokenizer usando pickle\n","with open('tokenizer.pkl', 'wb') as file:\n","    pickle.dump(tokenizer, file)"]},{"cell_type":"code","source":[],"metadata":{"id":"d4cTHGPt1f-B","executionInfo":{"status":"ok","timestamp":1719418559272,"user_tz":240,"elapsed":1929,"user":{"displayName":"Eliotness01","userId":"05711681705704863254"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1a173f8f-afde-45b9-f899-1319f04d01aa"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for pickle\u001b[0m\u001b[31m\n","\u001b[0m"]}]}]}